import streamlit as st
import pandas as pd
import requests
from datetime import datetime, date, timedelta
import time

# Fonction de d√©bogage simplifi√©e
def debug_log(message, data=None):
    if st.session_state.get("debug_mode", False):
        st.sidebar.markdown(f"**DEBUG:** {message}")
        if data is not None:
            st.sidebar.write(data)

# Fonction pour tester directement l'API
def test_api_connection():
    """Teste une connexion simple √† l'API"""
    try:
        # URL plus simple pour le test
        test_url = "https://data.economie.gouv.fr/api/v2/catalog/datasets/rappelconso-v2-gtin-espaces/records?limit=1"
        
        debug_log("Test de connexion API avec URL minimale", test_url)
        
        response = requests.get(test_url, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        if "records" in data and len(data["records"]) > 0:
            st.sidebar.success("‚úÖ Connexion √† l'API r√©ussie!")
            return True
        else:
            st.sidebar.warning("‚ö†Ô∏è Connexion API OK mais aucune donn√©e re√ßue")
            return True
        
    except Exception as e:
        st.sidebar.error(f"‚ùå Erreur de connexion API: {str(e)}")
        return False

# Fonction am√©lior√©e pour charger les donn√©es
def load_rappelconso_data():
    """Charge les donn√©es de l'API RappelConso avec une gestion d'erreurs am√©lior√©e"""
    
    # V√©rifier d'abord que l'API est accessible
    if not test_api_connection():
        st.error("Impossible de se connecter √† l'API RappelConso. V√©rifiez votre connexion internet.")
        return pd.DataFrame()
    
    try:
        # URL de base de l'API
        api_url = "https://data.economie.gouv.fr/api/v2/catalog/datasets/rappelconso-v2-gtin-espaces/records"
        
        # Dates pour le filtrage
        start_date = st.session_state.get("load_start_date", date(2022, 1, 1))
        if isinstance(start_date, str):
            start_date = datetime.strptime(start_date, "%Y-%m-%d").date()
        end_date = date.today()
        
        # Construction de la requ√™te (MODIFI√âE pour r√©soudre les probl√®mes)
        # 1. Utiliser des guillemets doubles pour les cha√Ænes
        # 2. Format de date ISO sans guillemets
        # 3. Retirer l'ordre de tri pour simplifier
        query_params = {
            # Solution 1: Utiliser le format q= avec une syntaxe plus simple
            "q": "Alimentation",  # Recherche simple par terme
            
            # Date range using q filter (technique alternative)
            # "refine.date_publication": f">={start_date.strftime('%Y-%m-%d')}",
            
            # Limit and offset still needed
            "limit": 100,
            "offset": 0
        }
        
        debug_log("Requ√™te API avec param√®tres simplifi√©s", query_params)
        
        # Effectuer la requ√™te
        with st.spinner("Test de r√©cup√©ration initiale de donn√©es..."):
            response = requests.get(api_url, params=query_params, timeout=30)
            response.raise_for_status()
            initial_data = response.json()
        
        # V√©rifier si nous avons des r√©sultats
        total_count = initial_data.get("total_count", 0)
        debug_log(f"Requ√™te de test r√©ussie. Total enregistrements: {total_count}")
        
        if total_count == 0:
            st.warning("Aucun rappel trouv√© avec la requ√™te de test.")
            return pd.DataFrame()
        
        # Si la requ√™te simple fonctionne, nous pouvons r√©cup√©rer plus de donn√©es
        all_records = []
        total_fetched = 0
        page_size = 100
        max_records = 5000  # Limiter le nombre total pour √©viter des probl√®mes de m√©moire
        
        # Initialiser la barre de progression
        progress_text = st.empty()
        progress_bar = st.progress(0)
        
        offset = 0
        while offset < min(total_count, max_records):
            query_params["offset"] = offset
            query_params["limit"] = page_size
            
            try:
                progress_text.text(f"Chargement des donn√©es {offset+1}-{min(offset+page_size, total_count)} sur {total_count}...")
                response = requests.get(api_url, params=query_params, timeout=30)
                response.raise_for_status()
                
                page_data = response.json()
                page_records = page_data.get("records", [])
                
                if not page_records:
                    debug_log(f"Pas de records √† l'offset {offset}", None)
                    break
                
                # Extraire les donn√©es des records
                for record in page_records:
                    if "record" in record and "fields" in record["record"]:
                        all_records.append(record["record"]["fields"])
                    else:
                        debug_log(f"Structure record inattendue √† l'index {len(all_records)}", record)
                
                total_fetched += len(page_records)
                progress_bar.progress(min(1.0, total_fetched / min(total_count, max_records)))
                
                offset += len(page_records)
                time.sleep(0.1)  # Pause courte pour √©viter de surcharger l'API
                
                if total_fetched >= max_records:
                    debug_log(f"Limite de {max_records} enregistrements atteinte")
                    break
                
            except Exception as e:
                debug_log(f"Erreur lors du chargement de la page √† l'offset {offset}", str(e))
                break
        
        # Effacer la barre de progression
        progress_text.empty()
        progress_bar.empty()
        
        # V√©rifier les r√©sultats
        if not all_records:
            st.error("Aucune donn√©e extraite des r√©ponses de l'API")
            return pd.DataFrame()
        
        # Cr√©er le dataframe
        df = pd.DataFrame(all_records)
        debug_log(f"DataFrame cr√©√© avec {len(df)} lignes", df.columns.tolist())
        
        # Normaliser les noms de colonnes
        column_mapping = {
            "categorie_produit": "categorie_de_produit",
            "sous_categorie_produit": "sous_categorie_de_produit",
            "marque_produit": "nom_de_la_marque_du_produit",
            "motif_rappel": "motif_du_rappel",
            "liens_vers_les_images": "liens_vers_images",
            "lien_vers_la_fiche_rappel": "lien_vers_la_fiche_rappel",
            "date_publication": "date_publication"
        }
        
        # Appliquer le mapping des colonnes existantes
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df[new_col] = df[old_col]
                if old_col != new_col:  # √âviter de supprimer une colonne qui vient d'√™tre cr√©√©e
                    df = df.drop(columns=[old_col])
        
        # V√©rifier si la colonne date_publication existe
        if "date_publication" in df.columns:
            try:
                # Convertir en datetime et filtrer par date
                df["date_publication"] = pd.to_datetime(df["date_publication"], errors="coerce")
                
                # Filtrer pour la plage de dates, si n√©cessaire
                start_date_dt = pd.Timestamp(start_date)
                end_date_dt = pd.Timestamp(end_date)
                df = df[(df["date_publication"] >= start_date_dt) & 
                         (df["date_publication"] <= end_date_dt)]
                
                # Convertir en date (sans heure)
                df["date_publication"] = df["date_publication"].dt.date
                
                # Tri par date
                df = df.sort_values("date_publication", ascending=False)
            except Exception as date_error:
                debug_log("Erreur lors du traitement des dates", str(date_error))
        
        # Afficher des statistiques de base
        st.success(f"‚úÖ {len(df)} rappels charg√©s avec succ√®s!")
        
        return df
        
    except requests.exceptions.HTTPError as http_err:
        st.error(f"Erreur HTTP: {http_err}")
        if hasattr(http_err, 'response'):
            debug_log("D√©tails de l'erreur HTTP", {
                "status_code": http_err.response.status_code,
                "headers": dict(http_err.response.headers),
                "content": http_err.response.text[:500] + "..." if len(http_err.response.text) > 500 else http_err.response.text
            })
        return pd.DataFrame()
    except Exception as e:
        st.error(f"Erreur lors du chargement des donn√©es: {str(e)}")
        return pd.DataFrame()

# Interface utilisateur simplifi√©e pour tester
st.title("Test de chargement API RappelConso")

# Activer le mode debug
if "debug_mode" not in st.session_state:
    st.session_state.debug_mode = True

# Param√®tres de chargement
st.header("üîç Param√®tres de Chargement")

# Date de d√©but
default_date = date(2022, 1, 1)
start_date = st.date_input("Charger les rappels depuis:", value=default_date)
st.session_state.load_start_date = start_date

# Bouton pour charger les donn√©es
if st.button("üîÑ Charger les donn√©es", type="primary"):
    df = load_rappelconso_data()
    
    if not df.empty:
        st.header("üìä Aper√ßu des donn√©es")
        st.write(f"Nombre total de rappels: {len(df)}")
        st.dataframe(df.head(10))
        
        # Afficher quelques statistiques
        if "categorie_de_produit" in df.columns:
            st.header("üìà Statistiques rapides")
            st.write("Top 5 des sous-cat√©gories:")
            if "sous_categorie_de_produit" in df.columns:
                st.write(df["sous_categorie_de_produit"].value_counts().head(5))
    else:
        st.error("Aucune donn√©e n'a pu √™tre charg√©e. Consultez les messages d'erreur.")

# Afficher la documentation de l'API
with st.expander("Documentation API RappelConso", expanded=False):
    st.markdown("""
    ## Informations sur l'API RappelConso
    
    L'API RappelConso est disponible via data.economie.gouv.fr et permet d'acc√©der aux donn√©es des rappels de produits.
    
    ### Points d'acc√®s
    
    - **API Explorer**: [https://data.economie.gouv.fr/explore/dataset/rappelconso-v2-gtin-espaces/api/](https://data.economie.gouv.fr/explore/dataset/rappelconso-v2-gtin-espaces/api/)
    - **API Records**: [https://data.economie.gouv.fr/api/v2/catalog/datasets/rappelconso-v2-gtin-espaces/records](https://data.economie.gouv.fr/api/v2/catalog/datasets/rappelconso-v2-gtin-espaces/records)
    
    ### Filtrage des donn√©es
    
    L'API permet plusieurs m√©thodes de filtrage:
    
    1. **Param√®tre `q`**: Recherche textuelle g√©n√©rale
    2. **Param√®tre `where`**: Filtre avec expressions SQL
    3. **Param√®tre `refine`**: Filtrage par valeur exacte
    
    ### Exemples
    
    ```
    # Recherche simple
    ?q=Alimentation
    
    # Filtre avec where
    ?where=categorie_produit="Alimentation"
    
    # Filtre avec refine
    ?refine.categorie_produit=Alimentation
    ```
    """)

# Afficher les options de d√©bogage
if st.session_state.debug_mode:
    with st.expander("Options de d√©bogage avanc√©es", expanded=False):
        if st.button("üß™ Test simple connexion API"):
            test_api_connection()
        
        if st.button("‚ùå Effacer le cache"):
            st.cache_data.clear()
            st.success("Cache effac√©!")
