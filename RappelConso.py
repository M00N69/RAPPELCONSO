import streamlit as st
import pandas as pd
import requests
from datetime import datetime, date, timedelta, timezone
import time
import plotly.express as px
import numpy as np
import math
from groq import Groq # Importez la librairie Groq
import json # Pour une gestion potentielle plus fine du JSON de l'√©chantillon

# --- Configuration de la page ---
st.set_page_config(
    page_title="RappelConso Insight",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- CSS simplifi√© pour le style de base ---
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');
    
    html, body, [class*="st-"] {
        font-family: 'Roboto', sans-serif;
    }

    .header {
        background: linear-gradient(135deg, #1a5276 0%, #2980b9 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin-bottom: 1.5rem;
        text-align: center;
        color: white;
    }
    .header h1 {
        font-size: 2.5em;
        margin-bottom: 0.5rem;
        font-weight: 700;
    }
    .header p {
        font-size: 1.1em;
        opacity: 0.9;
    }

    .card {
        background-color: white;
        border-radius: 8px;
        padding: 1.2rem;
        margin-bottom: 1rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        transition: transform 0.2s ease-in-out;
    }
    .card:hover {
        transform: translateY(-3px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }

    .metric {
        text-align: center;
        padding: 1rem;
        background-color: #e9ecef; /* Light grey background */
        border-radius: 8px;
        border-top: 3px solid #2980b9;
        margin-bottom: 1rem; /* Added margin */
    }
    .metric-value {
        font-size: 2.2em;
        font-weight: bold;
        color: #2980b9;
    }
    .metric div {
        font-size: 1em;
        color: #333; /* Darker text for label */
    }

    .recall-card {
        border-left: 4px solid #2980b9;
        padding-left: 0.8rem;
        height: 100%; /* Fill the column height */
    }
    
    .risk-high {
        color: #e74c3c; /* Red */
        font-weight: bold;
    }
    .risk-medium {
        color: #f39c12; /* Orange */
        font-weight: bold;
    }
    .risk-low {
        color: #27ae60; /* Green */
        font-weight: bold;
    }

    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }

    .stTabs [data-baseweb="tab"] {
        height: 50px;
        white-space: pre-wrap;
        background-color: #f0f0f0;
        border-radius: 4px 4px 0 0;
        gap: 10px;
        padding: 0px 20px;
    }

    .stTabs [data-baseweb="tab"] svg {
        color: #2980b9;
    }

    .stTabs [aria-selected="true"] {
        background-color: #2980b9;
        color: white;
        border-top: 3px solid #1a5276;
    }
    .stTabs [aria-selected="true"] svg {
        color: white;
    }

    .footer {
        margin-top: 3rem; /* Increased margin */
        padding-top: 1.5rem;
        border-top: 1px solid #ccc;
        text-align: center;
        color: #7f8c8d;
        font-size: 0.9em; /* Slightly larger font */
    }

    /* Adjust image container within card */
    .recall-card img {
         margin-top: 0px !important; /* Remove top margin added previously */
    }
     .recall-card div[data-testid="stImage"] {
        margin-top: 0px !important; /* Ensure no margin from streamlit image wrapper */
        margin-bottom: 10px; /* Add some space below the image */
    }

    /* Style for download button */
    .stDownloadButton > button {
        background-color: #27ae60; /* Green */
        color: white;
        border-radius: 8px;
        padding: 0.75rem 1.5rem;
        font-size: 1em;
    }
     .stDownloadButton > button:hover {
        background-color: #229954; /* Darker green */
        color: white;
    }

     /* Style for AI analysis button */
     .stButton button[data-testid^="stButton"] { # Use startswith for potential multiple buttons
         /* Example styling */
         background-color: #3498db; /* Blue */
         color: white;
     }
      .stButton button[data-testid^="stButton"]:hover {
         background-color: #2980b9; /* Darker blue */
         color: white;
     }


</style>
""", unsafe_allow_html=True)

# --- Mode debug ---
DEBUG = False # Set to True to enable debug logs

def debug_log(message, data=None):
    """Affiche des informations de d√©bogage"""
    if DEBUG:
        st.sidebar.markdown(f"**DEBUG:** {message}")
        if data is not None:
             # Use a more compact representation for debug data in sidebar
            if isinstance(data, (dict, list)):
                 try:
                      st.sidebar.json(data) # Use json for dict/list
                 except Exception as e:
                      st.sidebar.write(f"Cannot display data as JSON: {e}")
                      st.sidebar.write(data) # Fallback to writing raw data
            elif isinstance(data, pd.DataFrame):
                st.sidebar.dataframe(data.head()) # Use dataframe for pandas
            else:
                 st.sidebar.write(data) # Default write for other types


# --- Liste pr√©d√©finie de cat√©gories ---
# Fix√©e √† "alimentation" pour r√©pondre √† la demande.
CATEGORIES = ["alimentation"]

# --- Mod√®les Groq disponibles (liste mise √† jour) ---
# Mod√®les avec grande fen√™tre de contexte si disponibles
GROQ_MODELS = ["gemma2-9b-it", "llama-3.3-70b-versatile", "llama3-70b-8192", "deepseek-r1-distill-llama-70b", "llama3-8b-8192"] # Updated list

# --- Fonction de chargement des donn√©es avec cache ---
@st.cache_data(ttl=3600, show_spinner=False) # Cache pendant 1 heure, cache key d√©pend des param√®tres
def load_rappel_data(start_date: date = None, category: str = "alimentation", max_records: int = 1000):
    """
    Charge les donn√©es de l'API RappelConso.
    Applique le filtre de date et de cat√©gorie √† l'API.
    Force la cat√©gorie √† 'alimentation'.
    """
    api_url = "https://data.economie.gouv.fr/api/v2/catalog/datasets/rappelconso-v2-gtin-espaces/records"
    
    # On force la cat√©gorie √† 'alimentation' ici pour r√©pondre √† la demande sp√©cifique.
    actual_category_to_load = "alimentation" 

    params = {
        "limit": 100, # Limite par page, max 100 dans l'API
        "offset": 0
    }
    
    # Ajouter un filtre de cat√©gorie
    if actual_category_to_load:
        params["refine.categorie_produit"] = actual_category_to_load
    # Note: Pas de 'else' pour charger toutes les cat√©gories car la cat√©gorie est forc√©e.

    # Ajouter un filtre de date de publication si sp√©cifi√©
    if start_date:
         start_date_str = start_date.strftime("%Y-%m-%d")
         params["refine.date_publication"] = f">='{start_date_str}'"
    
    all_records = []
    total_count = 0
    
    # Indicateur de progression
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    try:
        # Premi√®re requ√™te pour estimer le nombre total
        initial_params = params.copy()
        initial_params["limit"] = 1
        
        status_text.text(f"Connexion √† l'API RappelConso (cat√©gorie: '{actual_category_to_load}')...")
        response = requests.get(api_url, params=initial_params, timeout=30)
        response.raise_for_status() # L√®ve une exception pour les erreurs HTTP
        
        data = response.json()
        total_count = data.get("total_count", 0)
        debug_log(f"Estimation Total Count API (cat: {actual_category_to_load}, date: >={start_date_str}): {total_count}", data)
        
        if total_count == 0:
            status_text.warning(f"Aucun rappel trouv√© avec les filtres initiaux (cat√©gorie: '{actual_category_to_load}').")
            progress_bar.progress(1.0)
            return pd.DataFrame()

        estimated_fetch_count = min(total_count, max_records)
        status_text.text(f"Chargement des donn√©es ({estimated_fetch_count} maximum)...")
        
        offset = 0
        # Ajuster la condition de boucle pour s'assurer de ne pas d√©passer max_records
        while offset < total_count and len(all_records) < max_records:
            params["offset"] = offset
            params["limit"] = min(100, max_records - len(all_records))

            if params["limit"] <= 0:
                 break

            response = requests.get(api_url, params=params, timeout=30)
            response.raise_for_status()
            
            page_data = response.json()
            records = page_data.get("records", [])
            
            if not records:
                break # Plus de donn√©es √† charger
            
            # Extraction des champs
            for record in records:
                if "record" in record and "fields" in record["record"]:
                    all_records.append(record["record"]["fields"])
            
            offset += len(records)
            # Mettre √† jour la progression bas√©e sur le nombre r√©el d'enregistrements collect√©s par rapport √† max_records
            progress_bar.progress(min(1.0, len(all_records) / max_records))
            
            time.sleep(0.05)
            
            if len(all_records) >= max_records:
                 status_text.info(f"Limite de {max_records} enregistrements atteinte.")
                 break

    except requests.exceptions.Timeout:
        status_text.error("Erreur: D√©lai d'attente d√©pass√© lors de la connexion √† l'API.")
        progress_bar.empty()
        return pd.DataFrame()
    except requests.exceptions.RequestException as e:
        status_text.error(f"Erreur API: Impossible de charger les donn√©es. {e}")
        debug_log("API Error details", e)
        progress_bar.empty()
        return pd.DataFrame()
    except Exception as e:
        status_text.error(f"Une erreur inattendue est survenue lors du chargement: {e}")
        debug_log("Loading Error details", e)
        progress_bar.empty()
        return pd.DataFrame()

    finally:
        if len(all_records) > 0 and progress_bar:
             progress_bar.progress(1.0)
        if status_text:
             status_text.empty() # Nettoyer le texte de statut
        if progress_bar:
             progress_bar.empty() # Nettoyer la barre de progression
    
    if not all_records:
        st.warning(f"Aucun rappel trouv√© avec les filtres sp√©cifi√©s (cat√©gorie: '{actual_category_to_load}').")
        return pd.DataFrame()
    
    # Cr√©er le DataFrame
    df = pd.DataFrame(all_records)
    
    # D√©bogage
    debug_log("DataFrame brut apr√®s chargement", df.head())
    
    # Standardiser les noms de colonnes pour l'interface et traiter les dates
    column_mapping = {
        "categorie_produit": "categorie",
        "sous_categorie_produit": "sous_categorie",
        "marque_produit": "marque",
        "modeles_ou_references": "modele",
        "motif_rappel": "motif",
        "risques_encourus": "risques",
        "distributeurs": "distributeurs",
        "liens_vers_les_images": "image_urls_raw", # Renomm√© pour clart√©
        "lien_vers_la_fiche_rappel": "fiche_url",
        "date_publication": "date_raw",  # Renomm√© pour clart√©
        "libelle": "nom",
        "id": "id",
        "zone_geographique_de_vente": "zone_vente",
        "conduites_a_tenir_par_le_consommateur": "conduites_a_tenir",
        "date_debut_commercialisation": "date_debut_commercialisation",
        "date_date_fin_commercialisation": "date_fin_commercialisation",
        "temperature_conservation": "temperature_conservation",
        "marque_salubrite": "marque_salubrite",
        "informations_complementaires": "informations_complementaires",
        "description_complementaire_risque": "description_complementaire_risque",
        "numero_contact": "numero_contact",
        "modalites_de_compensation": "modalites_de_compensation",
        "date_de_fin_de_la_procedure_de_rappel": "date_fin_procedure"
    }
    
    # Appliquer le renommage des colonnes existantes
    df.rename(columns=column_mapping, inplace=True)
    
    # Convertir la colonne date_raw en datetime et extraire la date pour affichage
    if "date_raw" in df.columns:
        # Convertir en datetime, coercer les erreurs en NaT (Not a Time) et s'assurer qu'ils sont en UTC
        df['date'] = pd.to_datetime(df['date_raw'], errors='coerce', utc=True)
        # Extraire la partie date pour l'affichage (ignorer le timezeone pour le conversion str)
        df['date_str'] = df['date'].dt.strftime('%Y-%m-%d')
    else:
        df['date'] = pd.NaT # Add dummy datetime col if date_raw is missing
        df['date_str'] = "Date manquante"

    # Extraire la premi√®re URL d'image si plusieurs sont pr√©sentes
    if "image_urls_raw" in df.columns:
         df['image_url'] = df['image_urls_raw'].apply(lambda x: str(x).split('|')[0].strip() if pd.notna(x) and str(x).strip() else None)
    else:
         df['image_url'] = None # Add dummy image col

    # Trier par date (plus r√©cent en premier) - G√©rer les NaT
    if "date" in df.columns:
        df = df.sort_values("date", ascending=False, na_position='last').reset_index(drop=True)

    # Assurer que seule la cat√©gorie 'alimentation' est pr√©sente si c'est la demande explicite
    # Ce filtre est redondant si le filtre API fonctionne, mais ajoute une s√©curit√©.
    # V√©rifier si la colonne 'categorie' existe avant de filtrer
    if 'categorie' in df.columns:
        df = df[df['categorie'] == 'alimentation'].reset_index(drop=True)
    else:
        st.warning("La colonne 'categorie' est absente des donn√©es. Le filtrage 'alimentation' ne peut pas √™tre appliqu√© apr√®s chargement.")

    # --- CORRECTION MAJEURE : Supprimer les doublons bas√©s sur l'ID ---
    # L'ID de la fiche ("id" ou "numero_fiche") devrait √™tre unique.
    # Si l'API retourne des doublons pour une raison quelconque, on les supprime ici.
    # Utiliser l'ID comme crit√®re de d√©duplication.
    initial_rows = len(df)
    if 'id' in df.columns:
         # IMPORTANT: reset_index() est n√©cessaire apr√®s drop_duplicates si vous comptez utiliser .iloc[] par la suite
         df.drop_duplicates(subset=['id'], keep='first', inplace=True)
         df.reset_index(drop=True, inplace=True) # Reset index after dropping rows
         if len(df) < initial_rows:
              st.sidebar.warning(f"Supprim√© {initial_rows - len(df)} doublons bas√©s sur l'ID.")
    else:
         st.warning("La colonne 'id' est absente des donn√©es. Impossible de supprimer les doublons bas√©s sur l'ID.")
         # Si 'id' est absent, tenter une d√©duplication sur un sous-ensemble de colonnes cl√©s
         key_cols_for_dedup = ['nom', 'marque', 'date_str', 'motif', 'risques']
         key_cols_existing = [col for col in key_cols_for_dedup if col in df.columns]
         if len(key_cols_existing) > 0:
              df.drop_duplicates(subset=key_cols_existing, keep='first', inplace=True)
              df.reset_index(drop=True, inplace=True) # Reset index after dropping rows
              if len(df) < initial_rows:
                   st.sidebar.warning(f"Supprim√© {initial_rows - len(df)} doublons bas√©s sur un ensemble de colonnes cl√©s.")


    # D√©bogage final
    debug_log("DataFrame trait√© et pr√™t", df.head())
    debug_log("Colonnes apr√®s traitement", list(df.columns))
    debug_log("Types de donn√©es", df.dtypes)
    if 'categorie' in df.columns:
        debug_log("Value counts 'categorie' (apr√®s filtre)", df['categorie'].value_counts())
    if 'sous_categorie' in df.columns:
        debug_log("Value counts 'sous_categorie'", df['sous_categorie'].value_counts().head())
    
    return df

# Fonction pour d√©terminer la classe de risque
def get_risk_class(risques):
    """D√©termine la classe CSS en fonction des risques mentionn√©s."""
    if isinstance(risques, str):
        risques_lower = risques.lower()
        if any(kw in risques_lower for kw in ["listeria", "salmonelle", "e. coli", "toxique", "dangereux", "mortel", "blessures graves", "contamination bact√©rienne"]):
            return "risk-high"
        elif any(kw in risques_lower for kw in ["allerg√®ne", "microbiologique", "corps √©tranger", "chimique", "blessures", "intoxication", "d√©passement de seuils", "non conforme", "additifs"]):
            return "risk-medium"
    return "risk-low" # Default or for less severe risks like odor/taste or minor defects

# Fonction pour afficher une carte de rappel (utilis√©e DANS une colonne)
def display_recall_card(row):
    """Affiche une carte de rappel dans le contexte d'une colonne."""
    
    # Extraire les donn√©es
    # Utiliser .get pour √©viter les KeyError si une colonne est manquante apr√®s chargement/filtrage
    nom = row.get("nom", row.get("modele", "Produit non sp√©cifi√©"))
    marque = row.get("marque", "Marque non sp√©cifi√©e")
    date_str = row.get("date_str", "Date non sp√©cifi√©e")
    categorie = row.get("categorie", "Cat√©gorie non sp√©cifi√©e")
    sous_categorie = row.get("sous_categorie", "Sous-cat√©gorie non sp√©cifi√©e")
    motif = row.get("motif", "Non sp√©cifi√©")
    risques = row.get("risques", "Non sp√©cifi√©")
    conduites = row.get("conduites_a_tenir", "Non sp√©cifi√©")
    distributeurs = row.get("distributeurs", "Non sp√©cifi√©")
    zone_vente = row.get("zone_vente", "Non sp√©cifi√©e")
    image_url = row.get("image_url")
    fiche_url = row.get("fiche_url", "#")
    
    # D√©terminer la classe de risque
    risk_class = get_risk_class(risques)
    
    # Afficher la carte
    # Utiliser st.container() √† l'int√©rieur de la colonne pour regrouper les √©l√©ments de la carte
    with st.container():
        st.markdown(f"""<div class="card recall-card">""", unsafe_allow_html=True)
        
        st.markdown(f"<h4>{nom}</h4>", unsafe_allow_html=True)
        st.markdown(f"<p><strong>Marque:</strong> {marque}</p>", unsafe_allow_html=True)
        st.markdown(f("<p><strong>Date de publication:</strong> {}</p>").format(date_str), unsafe_allow_html=True) 
        # CORRECTION : Syntaxe correcte des f-strings, corrig√©e ici
        st.markdown(f"<p><strong>Cat√©gorie:</strong> {categorie} > {sous_categorie}</p>", unsafe_allow_html=True) 
        st.markdown(f"<p><strong>Risques:</strong> <span class='{risk_class}'>{risques}</span></p>", unsafe_allow_html=True)
        
        # Afficher l'image si disponible, centr√©e dans la colonne
        if image_url:
            try:
                st.image(image_url, width=100)
            except Exception as e:
                debug_log(f"Error loading image {image_url}", e)
                st.info("Image non disponible")
        
        # Expander pour plus de d√©tails
        with st.expander("Voir plus de d√©tails"):
            st.markdown(f"<p><strong>Motif du rappel:</strong> {motif}</p>", unsafe_allow_html=True)
            st.markdown(f"<p><strong>Distributeurs:</strong> {distributeurs}</p>", unsafe_allow_html=True)
            st.markdown(f"<p><strong>Zone de vente:</strong> {zone_vente}</p>", unsafe_allow_html=True)
            st.markdown(f"<p><strong>Conduites √† tenir par le consommateur:</strong> {conduites}</p>", unsafe_allow_html=True)

            if fiche_url and fiche_url != "#":
                st.link_button("Voir la fiche compl√®te sur RappelConso", fiche_url, type="secondary")

        st.markdown(f"</div>", unsafe_allow_html=True)


# Helper function for pagination controls
def display_pagination_controls(current_page_state_key, total_items, items_per_page):
    """Affiche les boutons de pagination et le num√©ro de page."""
    # items_per_page est le nombre de cartes par page
    total_pages = math.ceil(total_items / items_per_page) if total_items > 0 else 1

    # S'assurer que la page actuelle ne d√©passe pas le total de pages (peut arriver apr√®s filtrage ou recherche)
    current_page = st.session_state.get(current_page_state_key, 1)
    if current_page > total_pages:
         st.session_state[current_page_state_key] = total_pages
    if current_page < 1:
         st.session_state[current_page_state_key] = 1

    # R√©cup√©rer la valeur corrig√©e pour l'affichage et les boutons
    current_page = st.session_state.get(current_page_state_key, 1)


    col_prev, col_info, col_next = st.columns([1, 3, 1])

    with col_prev:
        if current_page > 1:
            # Cl√© dynamique bas√©e sur le nom de l'√©tat de page pour √©viter les conflits
            if st.button("‚Üê Pr√©c√©dent", key=f"btn_prev_{current_page_state_key}"):
                st.session_state[current_page_state_key] = current_page - 1
                st.rerun()

    with col_info:
        st.markdown(f"<div style='text-align:center;'>Page {current_page} sur {total_pages}</div>", unsafe_allow_html=True)

    with col_next:
        if current_page < total_pages:
            # Cl√© dynamique bas√©e sur le nom de l'√©tat de page
            if st.button("Suivant ‚Üí", key=f"btn_next_{current_page_state_key}"):
                st.session_state[current_page_state_key] = current_page + 1
                st.rerun()

# Fonction pour interagir avec l'API Groq
def get_groq_response(api_key, model, prompt):
    """Envoie un prompt √† l'API Groq et retourne la r√©ponse."""
    if not api_key:
        return "Erreur : Cl√© API Groq non fournie."
    if not model:
        return "Erreur : Mod√®le Groq non s√©lectionn√©."
    if not prompt:
        return "Erreur : Aucune question pos√©e."

    # Nouveau System Prompt plus d√©taill√© pour mieux guider l'IA sur l'analyse de l'√©chantillon JSON
    # Augmentation de la clart√© sur l'analyse de l'√©chantillon pour les d√©tails sp√©cifiques
    system_prompt = """
    Vous √™tes un expert analyste en s√©curit√© alimentaire et en rappels de produits en France, sp√©cialis√© dans l'interpr√©tation des donn√©es de RappelConso.
    Votre t√¢che est de r√©pondre aux questions de l'utilisateur en vous basant *strictement* sur les donn√©es de rappels qui vous sont fournies dans le contexte.
    Les donn√©es sont un ensemble filtr√© de rappels de produits alimentaires. Le contexte inclut un r√©sum√© statistique g√©n√©ral et un √©chantillon de rappels individuels en format JSON.
    
    Consignes importantes :
    1.  **Basez-vous UNIQUEMENT** sur les donn√©es fournies dans le contexte (R√©sum√© des donn√©es et √âchantillon de rappels JSON). N'utilisez pas de connaissances externes.
    2.  Le **R√©sum√© des donn√©es** vous donne des statistiques g√©n√©rales (top motifs, top risques, etc.). C'est utile pour les tendances g√©n√©rales.
    3.  L'**√âchantillon de rappels (JSON)** liste les d√©tails *sp√©cifiques* (nom, marque, sous-cat√©gorie, motif) de chaque rappel de l'√©chantillon. Ce sont les donn√©es *brutes* que vous devez parcourir et analyser pour trouver des informations pr√©cises sur des cas particuliers ou des liens entre champs.
    4.  **POUR LES QUESTIONS SPECIFIQUES** (ex: quels produits sont associ√©s √† un risque particulier comme "bris de verre", quels distributeurs sont list√©s pour un motif donn√©, ou pour obtenir des d√©tails sur des rappels pr√©cis), **vous DEVEZ IMP√âRATIVEMENT ANALYSER attentivement l'√âchantillon de rappels (JSON)**. Parcourez les √©l√©ments de cet √©chantillon (chaque objet JSON dans la liste) et examinez attentivement les champs comme 'nom', 'marque', 'motif', 'sous_categorie', etc. Recherchez les termes cl√©s de la question (ex: "verre", "m√©tal", "listeria") dans ces champs.
    5.  Si vous trouvez des correspondances dans l'√©chantillon JSON, **liste les produits sp√©cifiques trouv√©s dans l'√©chantillon** qui correspondent √† la demande. Indiquez leur nom, marque, sous-cat√©gorie et motif pour chaque produit trouv√©. Si plusieurs produits sont trouv√©s, listez-les clairement.
    6.  Si une question ne peut pas √™tre r√©pondue avec les donn√©es fournies (parce que l'information n'appara√Æt pas du tout dans le r√©sum√© ET n'appara√Æt pas dans l'√©chantillon JSON), dites-le clairement (ex: "Je ne dispose pas d'informations suffisantes dans les donn√©es fournies pour r√©pondre pr√©cis√©ment √† cette question. L'information [mentionner le type d'information recherch√©e, ex: "sur les rappels li√©s au verre"] n'appara√Æt pas dans l'√©chantillon de rappels disponible pour l'analyse.").
    7.  Structurez votre r√©ponse de mani√®re claire, en utilisant des tirets, des listes ou des paragraphes courts. Commencez par r√©pondre directement √† la question si possible, puis ajoutez des pr√©cisions ou des limitations bas√©es sur les donn√©es.
    8.  Soyez concis et allez droit au but.
    9.  Ne mentionnez pas directement le format JSON ou les "indices" de l'√©chantillon dans votre r√©ponse √† l'utilisateur. Pr√©sentez les informations de mani√®re naturelle, comme si vous aviez lu les fiches de rappel.
    10. Les colonnes importantes pour l'analyse sont : 'nom', 'marque', 'sous_categorie', 'motif'. Concentrez votre analyse sur celles-ci dans l'√©chantillon JSON. Les autres colonnes du DataFrame ('risques', 'date_str', 'zone_vente', 'distributeurs', 'conduites_a_tenir') sont incluses dans le r√©sum√© mais pas dans l'√©chantillon JSON pour r√©duire la taille.
    """

    try:
        client = Groq(api_key=api_key)

        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": system_prompt # Utilise le nouveau system prompt
                },
                {
                    "role": "user",
                    "content": prompt, # Le prompt inclut toujours le contexte des donn√©es + la question utilisateur
                }
            ],
            model=model,
            temperature=0.1, # Temp√©rature basse pour des r√©ponses factuelles
            max_tokens=3000, # Augmente l√©g√®rement les tokens max pour une r√©ponse plus compl√®te si n√©cessaire et compte tenu de la plus grande fen√™tre de contexte de certains mod√®les
        )
        return chat_completion.choices[0].message.content

    except Exception as e:
        debug_log("Erreur API Groq", e)
        # Cacher la cl√© API dans le message d'erreur public
        error_message = str(e).replace(api_key, "[Votre Cl√© API]")
        return f"Une erreur est survenue lors de l'appel √† l'API Groq : {error_message}"

# Fonction pour pr√©parer le contexte des donn√©es filtr√©es pour l'IA
def prepare_data_context(df_filtered):
    """
    Pr√©pare un r√©sum√© textuel et un √©chantillon de donn√©es brutes
    des donn√©es filtr√©es pour le mod√®le IA.
    """
    if df_filtered.empty:
        return "Aucune donn√©e de rappel n'est disponible pour l'analyse."

    total_count = len(df_filtered)
    date_min = df_filtered['date_str'].min() if 'date_str' in df_filtered.columns and not df_filtered['date_str'].isna().all() else "N/A"
    date_max = df_filtered['date_str'].max() if 'date_str' in df_filtered.columns and not df_filtered['date_str'].isna().all() else "N/A"

    context_summary = f"Analyse bas√©e sur {total_count} rappels de produits alimentaires publi√©s entre le {date_min} et le {date_max}. "

    # Ajouter les informations des top N pour les colonnes pertinentes s'ils existent et ne sont pas vides apr√®s filtrage
    if 'sous_categorie' in df_filtered.columns and not df_filtered['sous_categorie'].dropna().empty:
        top_subcats = df_filtered['sous_categorie'].value_counts().nlargest(5).to_dict()
        context_summary += f"\nR√©sum√© des sous-cat√©gories les plus fr√©quentes : {top_subcats}. " # Clarifi√© "R√©sum√©"

    if 'motif' in df_filtered.columns and not df_filtered['motif'].dropna().empty:
        top_motifs = df_filtered['motif'].value_counts().nlargest(5).to_dict()
        context_summary += f"\nR√©sum√© des motifs de rappel les plus fr√©quents : {top_motifs}. " # Clarifi√© "R√©sum√©"

    if 'risques' in df_filtered.columns and not df_filtered['risques'].dropna().empty:
        top_risks = df_filtered['risques'].value_counts().nlargest(5).to_dict()
        context_summary += f"\nR√©sum√© des risques les plus fr√©quents : {top_risks}. " # Clarifi√© "R√©sum√©"
        
    if 'marque' in df_filtered.columns and not df_filtered['marque'].dropna().empty:
        top_brands = df_filtered['marque'].value_counts().nlargest(5).to_dict()
        context_summary += f"\nR√©sum√© des marques avec le plus de rappels : {top_brands}. " # Clarifi√© "R√©sum√©"

    # Inclure un √©chantillon des donn√©es brutes (limiter le nombre de lignes et les colonnes)
    # S√©lectionner les colonnes sp√©cifi√©es pour l'√©chantillon JSON
    relevant_cols_for_sample = ['nom', 'marque', 'sous_categorie', 'motif'] # Colonnes limit√©es comme demand√©

    # Filtrer pour garder uniquement les colonnes qui existent dans df_filtered
    relevant_cols_existing_in_filtered = [col for col in relevant_cols_for_sample if col in df_filtered.columns]

    # Taille de l'√©chantillon JSON augment√©e pour les mod√®les avec grande fen√™tre de contexte
    # Attention : m√™me avec une grande fen√™tre, 200 rappels peuvent √™tre volumineux. R√©duire si les erreurs 413 persistent.
    sample_size = min(len(df_filtered), 200) # Maintien l'√©chantillon √† 200

    # Utiliser head(sample_size) pour les plus r√©cents et s√©lectionner les colonnes pertinentes pour l'√©chantillon
    df_sample = df_filtered[relevant_cols_existing_in_filtered].head(sample_size) 

    context_sample = ""
    if not df_sample.empty:
        # Convertir explicitement l'√©chantillon entier en cha√Æne de caract√®res pour √©viter les erreurs de s√©rialisation
        df_sample_str = df_sample.astype(str) 
        
        context_sample = f"\n\n√âchantillon de {len(df_sample_str)} rappels (JSON) :\n" # Indique la taille de l'√©chantillon
        # Convertir l'√©chantillon en string JSON.
        try:
            list_of_dicts = df_sample_str.to_dict(orient='records')
            # Utilisation de json.dumps pour plus de contr√¥le et de robustesse
            context_sample += json.dumps(list_of_dicts, indent=2, ensure_ascii=False)

        except Exception as e:
             debug_log("Error converting sample to JSON after astype(str)", e)
             context_sample = f"\n√âchantillon de rappels (JSON) : √âchantillon non disponible en raison d'une erreur de formatage ({e}).\n" 


    full_context = context_summary + context_sample

    return full_context

# --- Fonction principale ---
def main():
    # Initialiser les √©tats de session n√©cessaires avant toute utilisation
    # Cette initialisation en d√©but de main() est la plus fiable dans Streamlit
    if "rappel_data" not in st.session_state: st.session_state.rappel_data = None
    if "load_params" not in st.session_state: st.session_state.load_params = None
    if "current_page" not in st.session_state: st.session_state.current_page = 1
    if "search_current_page" not in st.session_state: st.session_state.search_current_page = 1
    if "selected_subcategories" not in st.session_state: st.session_state.selected_subcategories = ["Toutes"]
    if "selected_brands" not in st.session_state: st.session_state.selected_brands = ["Toutes"]
    if "selected_risks" not in st.session_state: st.session_state.selected_risks = ["Tous"]
    if "quick_search_input" not in st.session_state: st.session_state.quick_search_input = ""
    # Correction ici : utiliser une cl√© diff√©rente pour l'√©tat de la valeur du widget
    if "last_search_term_widget_value" not in st.session_state:
         st.session_state.last_search_term_widget_value = ""
    if "items_per_page_main" not in st.session_state: st.session_state.items_per_page_main = 10
    if "items_per_page_search" not in st.session_state: st.session_state.items_per_page_search = 10
    if "groq_api_key" not in st.session_state: st.session_state.groq_api_key = ""
    # Correction ici : s'assurer que l'√©tat groq_model est un mod√®le valide d√®s l'initialisation
    if "groq_model" not in st.session_state or (st.session_state.groq_model is not None and st.session_state.groq_model not in GROQ_MODELS):
        st.session_state.groq_model = GROQ_MODELS[0] if GROQ_MODELS else None # Utiliser le premier mod√®le comme d√©faut si la liste n'est pas vide
    if "ai_question" not in st.session_state: st.session_state.ai_question = ""
    if "ai_response" not in st.session_state: st.session_state.ai_response = ""
    if "last_ai_question" not in st.session_state: st.session_state.last_ai_question = ""


    # En-t√™te (identique)
    st.markdown("""
    <div class="header">
        <h1>RappelConso Insight</h1>
        <p>Analyse des rappels de produits en France</p>
    </div>
    """, unsafe_allow_html=True)
    
    # --- Sidebar pour les param√®tres ---
    st.sidebar.title("Param√®tres")

    st.sidebar.subheader("Chargement des donn√©es")
    # S√©lection de la cat√©gorie (fixe √† alimentation selon la demande)
    st.sidebar.write("Cat√©gorie charg√©e: **alimentation**")
    selected_category_loading = "alimentation" # <--- Fixe la cat√©gorie pour le chargement

    # Date de d√©but
    start_date = st.sidebar.date_input(
        "Rappels depuis la date:",
        value=date.today() - timedelta(days=180), # Par d√©faut, 6 mois avant aujourd'hui
        max_value=date.today()
    )
    
    # Nombre max de rappels
    max_records = st.sidebar.slider(
        "Nombre max d'enregistrements √† charger:", 
        min_value=100,
        max_value=10000, # Augment√© la limite max
        value=2000,
        step=100
    )
    
    # Bouton pour charger les donn√©es
    load_button = st.sidebar.button("Charger/Actualiser les donn√©es", type="primary")

    # D√©terminer si les donn√©es doivent √™tre charg√©es (au premier lancement ou si les param√®tres de chargement ont chang√©)
    current_load_params = (selected_category_loading, start_date, max_records)
    
    # Charger les donn√©es si le bouton est cliqu√© OU si c'est la premi√®re ex√©cution ET qu'aucun param√®tre n'est enregistr√©
    if load_button or (st.session_state.rappel_data is None and st.session_state.load_params is None):
        # Afficher un message pendant le chargement
        status_message = st.sidebar.info("Chargement des donn√©es en cours...")
        st.session_state.rappel_data = load_rappel_data(
            start_date=start_date,
            category=selected_category_loading, # Utilise la cat√©gorie s√©lectionn√©e/fix√©e
            max_records=max_records
        )
        st.session_state.load_params = current_load_params # Sauvegarder les param√®tres utilis√©s

        # R√©initialiser les filtres d'analyse post-chargement et pagination apr√®s un nouveau chargement
        st.session_state.selected_subcategories = ["Toutes"]
        st.session_state.selected_brands = ["Toutes"]
        st.session_state.selected_risks = ["Tous"]
        st.session_state.current_page = 1 # Reset main list pagination
        st.session_state.search_current_page = 1 # Reset search results pagination
        st.session_state.last_search_term_widget_value = "" # Reset search term state for pagination check
        st.session_state.quick_search_input = "" # Clear search input widget via session state
        # R√©initialiser la r√©ponse IA et la question
        st.session_state.ai_response = ""
        st.session_state.last_ai_question = ""


        status_message.empty() # Clear the loading message (if it was there)
        # Use st.toast or st.success for feedback after loading
        if st.session_state.rappel_data is not None and not st.session_state.rappel_data.empty:
            st.toast("Donn√©es charg√©es avec succ√®s !", icon="‚úÖ")
        else:
             st.toast("Aucune donn√©e charg√©e avec les param√®tres sp√©cifi√©s.", icon="‚ö†Ô∏è")

        st.rerun() # Rerun pour appliquer le chargement et afficher les donn√©es

    # V√©rifier si les donn√©es sont charg√©es
    if st.session_state.rappel_data is None or st.session_state.rappel_data.empty:
        if st.session_state.load_params is not None: # Only show message if a load was attempted
             st.info("Aucun rappel ne correspond aux filtres de chargement s√©lectionn√©s, ou veuillez charger les donn√©es en cliquant sur le bouton dans la barre lat√©rale.")
        return
    
    df = st.session_state.rappel_data.copy() # Utiliser une copie pour les filtrages post-chargement

    # --- Filtres post-chargement (dans la barre lat√©rale) ---
    st.sidebar.markdown("---")
    st.sidebar.subheader("Filtres d'analyse")

    # Filtre Sous-cat√©gorie
    # Utiliser un set pour les options pour √©viter les doublons m√™me si dropna est appel√©
    all_subcategories = ["Toutes"] + sorted(set(df["sous_categorie"].dropna().tolist()))
    selected_subcategories = st.sidebar.multiselect(
        "Filtrer par sous-cat√©gorie:",
        options=all_subcategories,
        default=st.session_state.selected_subcategories,
        key="sidebar_subcat_filter" # Cl√© unique
    )
    st.session_state.selected_subcategories = selected_subcategories # Sauvegarder la s√©lection


    # Filtre Marque
    all_brands = ["Toutes"] + sorted(set(df["marque"].dropna().tolist()))
    selected_brands = st.sidebar.multiselect(
        "Filtrer par marque:",
        options=all_brands,
        default=st.session_state.selected_brands,
        key="sidebar_brand_filter" # Cl√© unique
    )
    st.session_state.selected_brands = selected_brands # Sauvegarder la s√©lection


    # Filtre Risque
    all_risks = ["Tous"] + sorted(set(df["risques"].dropna().tolist()))
    selected_risks = st.sidebar.multiselect(
        "Filtrer par risques encourus:",
        options=all_risks,
        default=st.session_state.selected_risks,
        key="sidebar_risk_filter" # Cl√© unique
    )
    st.session_state.selected_risks = selected_risks # Sauvegarder la s√©lection

    
    # Appliquer les filtres post-chargement pour obtenir le DataFrame affich√©
    df_filtered = df.copy()

    if "Toutes" not in selected_subcategories:
        # Utiliser isin et une liste pour g√©rer les NaN correctement si besoin, mais dropna() sur les options devrait suffire
        df_filtered = df_filtered[df_filtered["sous_categorie"].isin(selected_subcategories)]

    if "Toutes" not in selected_brands:
         df_filtered = df_filtered[df_filtered["marque"].isin(selected_brands)]

    if "Tous" not in selected_risks:
         df_filtered = df_filtered[df_filtered["risques"].isin(selected_risks)]

    # V√©rifier si le DataFrame filtr√© est vide (important pour la suite)
    if df_filtered.empty:
        st.warning("Aucun rappel ne correspond aux filtres d'analyse s√©lectionn√©s.")
        # Afficher les m√©triques bas√©es sur le df *charg√©* quand m√™me pour information
        col1, col2, col3, col4 = st.columns(4)
        with col1: st.markdown(f"""<div class="metric"><div class="metric-value">{len(df)}</div><div>Rappels charg√©s</div></div>""", unsafe_allow_html=True)
        with col2: st.markdown(f"""<div class="metric"><div class="metric-value">0</div><div>Rappels affich√©s</div></div>""", unsafe_allow_html=True)
        with col3: st.markdown(f"""<div class="metric"><div class="metric-value">0</div><div>Rappels (30 derniers jours)</div></div>""", unsafe_allow_html=True)
        with col4: st.markdown(f"""<div class="metric"><div class="metric-value">0</div><div>Marques uniques</div></div>""", unsafe_allow_html=True)
        
        # Afficher les onglets mais avec des messages d'absence de donn√©es √† l'int√©rieur
        tab1, tab2, tab3, tab4 = st.tabs(["üìã Liste des rappels", "üìä Visualisations", "üîç Recherche rapide", "ü§ñ Analyse IA"])
        with tab1: st.info("Aucun rappel √† afficher avec les filtres s√©lectionn√©s.")
        with tab2: st.info("Aucune donn√©e √† visualiser avec les filtres s√©lectionn√©s.")
        with tab3: st.info("Aucune donn√©e √† rechercher avec les filtres s√©lectionn√©s.")
        with tab4: st.info("Aucune donn√©e disponible pour l'analyse IA avec les filtres s√©lectionn√©s.")

        # R√©initialiser la r√©ponse IA si le filtre devient vide
        st.session_state.ai_response = ""
        st.session_state.last_ai_question = ""

        return # Arr√™ter l'ex√©cution de main() ici si df_filtered est vide


    # --- Afficher quelques m√©triques ---
    st.subheader("Vue d'overview") # Changed title to avoid confusion with "Analyse IA"
    col1, col2, col3, col4 = st.columns(4)
    
    # Nombre total de rappels charg√©s (based on the initial API load)
    with col1:
        st.markdown(f"""
        <div class="metric">
            <div class="metric-value">{len(df)}</div>
            <div>Rappels charg√©s</div>
        </div>
        """, unsafe_allow_html=True)
    
    # Nombre de rappels filtr√©s actuellement affich√©s (based on df_filtered)
    with col2:
         st.markdown(f"""
         <div class="metric">
             <div class="metric-value">{len(df_filtered)}</div>
             <div>Rappels affich√©s</div>
         </div>
         """, unsafe_allow_html=True)

    # Rappels r√©cents (calcul√©s sur les donn√©es filtr√©es)
    # Utiliser la colonne 'date' qui est datetime
    today = datetime.now(tz=timezone.utc) # Comparer avec un datetime UTC
    # Filtrer les dates non valides (NaT) avant la comparaison
    recent_recalls = df_filtered[df_filtered['date'].notna() & (df_filtered['date'] >= (today - timedelta(days=30)))]
    recent_count = len(recent_recalls)
    
    with col3:
        st.markdown(f"""
        <div class="metric">
            <div class="metric-value">{recent_count}</div>
            <div>Rappels (30 derniers jours)</div>
        </div>
        """, unsafe_allow_html=True)
    
    # Nombre de marques uniques dans les donn√©es filtr√©es
    if "marque" in df_filtered.columns:
        # Ajouter dropna() pour compter uniquement les marques non nulles
        brand_count_filtered = df_filtered["marque"].dropna().nunique()
        with col4:
            st.markdown(f"""
            <div class="metric">
                <div class="metric-value">{brand_count_filtered}</div>
                <div>Marques uniques (filtr√©es)</div>
            </div>
            """, unsafe_allow_html=True)
    else:
        with col4:
             st.markdown(f"""
             <div class="metric">
                 <div class="metric-value">N/A</div>
                 <div>Marques uniques</div>
             </div>
             """, unsafe_allow_html=True)

    # --- Bouton de t√©l√©chargement pour les donn√©es filtr√©es ---
    if not df_filtered.empty:
         st.markdown("---") # S√©parateur visuel
         # Correction : G√©n√©rer le CSV √† la demande dans le lambda pour s'assurer qu'il est √† jour
         @st.cache_data(ttl=60) # Cache le CSV pour 60s pour √©viter de le reg√©n√©rer √† chaque rerun minor
         def convert_df_to_csv(df):
             # Utiliser .copy() ici avant to_csv pour √©viter les SettingWithCopyWarning potentielles
             # si df_filtered_copy √©tait modifi√©e avant le to_csv
             return df.to_csv(index=False).encode('utf-8')

         csv_data = convert_df_to_csv(df_filtered)

         st.download_button(
             label="T√©l√©charger les donn√©es filtr√©es (CSV)",
             data=csv_data,
             file_name=f'rappelconso_alimentation_depuis_{start_date.strftime("%Y-%m-%d")}_filtered.csv',
             mime='text/csv',
             key="download_filtered_csv" # Cl√© unique
         )
         st.markdown("---") # S√©parateur visuel


    # --- Section Analyse IA dans la Sidebar ---
    st.sidebar.markdown("---")
    st.sidebar.subheader("ü§ñ Analyse IA")
    st.sidebar.info("Analysez les donn√©es filtr√©es √† l'aide de l'IA Groq.")

    # Input Cl√© API Groq
    groq_api_key = st.sidebar.text_input(
        "Votre cl√© API Groq :",
        type="password",
        value=st.session_state.groq_api_key,
        key="groq_api_key_input", # Cl√© unique
        help="Entrez votre cl√© API Groq (vous pouvez l'obtenir sur console.groq.com). La cl√© n'est pas enregistr√©e durablement."
    )
    st.session_state.groq_api_key = groq_api_key # Sauvegarder dans l'√©tat de session

    # S√©lection du mod√®le Groq
    groq_model = st.sidebar.selectbox(
        "Choisissez le mod√®le :",
        options=GROQ_MODELS,
        index=GROQ_MODELS.index(st.session_state.groq_model) if st.session_state.groq_model in GROQ_MODELS else 0,
        key="groq_model_select", # Cl√© unique
        help="S√©lectionnez le mod√®le d'IA Groq √† utiliser. Les mod√®les avec grande fen√™tre de contexte (ex: llama-3.3-70b-versatile) sont meilleurs pour analyser l'√©chantillon JSON."
    )
    st.session_state.groq_model = groq_model # Sauvegarder dans l'√©tat de session

    # Champ de texte pour la question de l'utilisateur
    ai_question = st.sidebar.text_area(
        "Posez une question sur les donn√©es filtr√©es :",
        value=st.session_state.ai_question,
        key="ai_question_input", # Cl√© unique
        height=150,
        placeholder="Ex: Quels sont les principaux produits qui ont eu un rappel pour pr√©sence de verre ? Y a-t-il des tendances r√©centes ?",
        help="L'IA analysera les donn√©es actuellement affich√©es (filtr√©es)."
    )
    st.session_state.ai_question = ai_question # Sauvegarder dans l'√©tat de session

    # Bouton pour d√©clencher l'analyse
    analyze_button = st.sidebar.button("Analyser avec l'IA", type="secondary", key="analyze_ai_button")

    # --- Logic to trigger AI analysis ---
    if analyze_button:
         if not groq_api_key:
             st.sidebar.error("Veuillez entrer votre cl√© API Groq.")
         elif not groq_model:
             st.sidebar.error("Veuillez s√©lectionner un mod√®le Groq.")
         elif not ai_question:
             st.sidebar.warning("Veuillez poser une question.")
         # La v√©rification de df_filtered.empty est faite plus haut dans main()
         # et le return arr√™te l'ex√©cution si les donn√©es filtr√©es sont vides.
         else:
             # Pr√©parer le prompt pour l'IA
             data_context = prepare_data_context(df_filtered)
             # Le prompt inclut le system prompt (dans get_groq_response) et ce contenu utilisateur
             full_prompt_content = f"Donn√©es contextuelles sur les rappels :\n{data_context}\n\nQuestion de l'utilisateur :\n{ai_question}\n\nR√©ponse :"

             # Appeler l'API Groq avec un spinner
             with st.spinner("L'IA analyse les donn√©es..."):
                 st.session_state.ai_response = get_groq_response(groq_api_key, groq_model, full_prompt_content)
                 st.session_state.last_ai_question = ai_question # Sauvegarder la question pos√©e

             st.toast("Analyse IA termin√©e !", icon="ü§ñ")
             # Rerun pour mettre √† jour l'affichage principal avec la r√©ponse
             st.rerun()


    # --- Afficher les onglets (avec le nouvel onglet IA) ---
    tab1, tab2, tab3, tab4 = st.tabs(["üìã Liste des rappels", "üìä Visualisations", "üîç Recherche rapide", "ü§ñ Analyse IA"])
    
    # Stocker l'onglet actif si vous voulez qu'il reste s√©lectionn√©
    # current_active_tab = st.session_state.get("current_tab", "Liste des rappels")
    # Assigner le contenu aux onglets...

    with tab1:
        # ... (le contenu de l'onglet Liste des rappels) ...
        st.subheader("Liste des rappels filtr√©s")
        
        items_per_page_main = st.select_slider(
            "Rappels par page:",
            options=[5, 10, 20, 50],
            value=st.session_state.items_per_page_main, # Utiliser l'√©tat de session
            key="slider_items_per_page_main"
        )
        st.session_state.items_per_page_main = items_per_page_main

        # Recalculer le hash de pagination si les filtres ou le nombre d'items par page changent
        current_pagination_state_main = hash((
            tuple(st.session_state.selected_subcategories),
            tuple(st.session_state.selected_brands),
            tuple(st.session_state.selected_risks),
            items_per_page_main
        ))
        
        # Comparer le hash actuel avec le dernier hash enregistr√© pour la pagination principale
        # Utiliser .get() avec un d√©faut pour √©viter les erreurs au tout premier run
        if st.session_state.get("pagination_state_main") != current_pagination_state_main:
             st.session_state.current_page = 1 # R√©initialiser √† la page 1 si les filtres/items par page changent
             st.session_state.pagination_state_main = current_pagination_state_main # Mettre √† jour le hash enregistr√©

        total_items_main = len(df_filtered)
        
        # Afficher les rappels de la page actuelle en 2 colonnes
        start_idx_main = (st.session_state.current_page - 1) * items_per_page_main
        end_idx_main = min(start_idx_main + items_per_page_main, total_items_main)

        if total_items_main > 0:
            # CORRECTION: Obtenir la sous-section du DataFrame pour la page actuelle
            # Utiliser .iloc pour la s√©lection par position, puis reset_index pour avoir des indices 0, 1, 2... sur la page
            # .copy() est une bonne pratique ici pour √©viter les SettingWithCopyWarning potentielles
            page_df_main = df_filtered.iloc[start_idx_main:end_idx_main].copy().reset_index(drop=True) 
            
            # It√©rer sur les indices locaux de cette sous-section (0, 1, 2...) par pas de 2
            # len(page_df_main) donne le nombre d'√©l√©ments sur la page actuelle
            for i in range(0, len(page_df_main), 2):
                col1, col2 = st.columns(2)
                
                # Afficher le premier produit de la paire en utilisant l'index local 'i' de page_df_main
                with col1:
                    # Gr√¢ce √† reset_index(), l'index 'i' est valide si i < len(page_df_main)
                    display_recall_card(page_df_main.iloc[i])
                
                # Afficher le deuxi√®me produit si il existe dans la sous-section
                if i + 1 < len(page_df_main):
                    with col2:
                         display_recall_card(page_df_main.iloc[i+1])
                # else:
                #    avec col2: st.empty() # Streamlit g√®re les colonnes vides automatiquement

            display_pagination_controls("current_page", total_items_main, items_per_page_main)

        # else: (message d√©j√† g√©r√© par le bloc 'if df_filtered.empty' plus haut)


    with tab2:
        # Contenu de l'onglet Visualisations (identique, utilise les fonctions modifi√©es)
        st.subheader("Visualisations des donn√©es filtr√©es")
        
        st.write("### √âvolution des rappels par mois")
        df_filtered_valid_dates = df_filtered.dropna(subset=['date'])
        if "date" in df_filtered_valid_dates.columns and not df_filtered_valid_dates.empty:
            try:
                # Grouper par mois en utilisant la colonne datetime.dt.to_period('M')
                # et convertir en string pour l'axe Plotly.
                monthly_counts = df_filtered_valid_dates.groupby(df_filtered_valid_dates['date'].dt.to_period('M')).size().reset_index(name="count")
                monthly_counts["month_str"] = monthly_counts["date"].astype(str)
                monthly_counts = monthly_counts.sort_values("month_str")
                if not monthly_counts.empty:
                    fig_time = px.line(monthly_counts, x="month_str", y="count", title="Nombre de rappels par mois", labels={"month_str": "Mois", "count": "Nombre de rappels"}, markers=True)
                    fig_time.update_layout(xaxis_title="Mois", yaxis_title="Nombre de rappels", hovermode="x unified")
                    st.plotly_chart(fig_time, use_container_width=True)
                else: st.warning("Pas assez de donn√©es temporelles valides pour cr√©er un graphique.")
            except Exception as e: st.error(f"Erreur lors de la cr√©ation du graphique temporel: {str(e)}"); debug_log("Time series chart error", e)
        else: st.info("La colonne de date n'est pas disponible ou contient des valeurs manquantes dans les donn√©es filtr√©es.")

        if "sous_categorie" in df_filtered.columns:
            st.write("### R√©partition par sous-cat√©gorie")
            valid_subcats = df_filtered["sous_categorie"].dropna()
            if not valid_subcats.empty:
                num_top_subcats = st.slider("Nombre de sous-cat√©gories √† afficher:", 5, 30, 10, key="subcat_slider")
                top_subcats = valid_subcats.value_counts().nlargest(num_top_subcats)
                fig_subcat = px.pie(values=top_subcats.values, names=top_subcats.index, title=f"Top {num_top_subcats} des sous-cat√©gories", hole=0.3)
                fig_subcat.update_traces(textposition='inside', textinfo='percent+label')
                st.plotly_chart(fig_subcat, use_container_width=True)
            else: st.info("Aucune donn√©e de sous-cat√©gorie disponible dans les donn√©es filtr√©es.")

        if "risques" in df_filtered.columns:
            st.write("### R√©partition par type de risque")
            valid_risks = df_filtered["risques"].dropna()
            if not valid_risks.empty:
                risk_counts = valid_risks.value_counts().reset_index()
                risk_counts.columns = ["risk", "count"]
                risk_counts['risk_level'] = risk_counts['risk'].apply(get_risk_class)
                risk_counts['risk_level_label'] = risk_counts['risk_level'].map({'risk-high': 'Risque √âlev√©', 'risk-medium': 'Risque Moyen', 'risk-low': 'Risque Faible'})
                risk_level_order = ['Risque √âlev√©', 'Risque Moyen', 'Risque Faible']
                risk_counts['risk_level_label'] = pd.Categorical(risk_counts['risk_level_label'], categories=risk_level_order, ordered=True)
                risk_counts = risk_counts.sort_values(['risk_level_label', 'count'], ascending=[True, False])
                fig_risks = px.bar(risk_counts, x="risk", y="count", title="R√©partition par type de risque", labels={"risk": "Type de risque", "count": "Nombre de rappels", "risk_level_label": "Niveau de risque"}, color="risk_level_label", color_discrete_map={'Risque √âlev√©': '#e74c3c', 'Risque Moyen': '#f39c12', 'Risque Faible': '#27ae60'})
                fig_risks.update_layout(xaxis_tickangle=-45)
                st.plotly_chart(fig_risks, use_container_width=True)
            else: st.info("Aucune donn√©e de risque disponible dans les donn√©es filtr√©es.")

        if "marque" in df_filtered.columns:
             st.write("### Marques les plus fr√©quentes")
             valid_brands = df_filtered["marque"].dropna()
             if not valid_brands.empty:
                 num_top_brands = st.slider("Nombre de marques √† afficher:", 5, 30, 10, key="top_brands_slider")
                 top_brands = valid_brands.value_counts().nlargest(num_top_brands)
                 fig_brands = px.bar(x=top_brands.index, y=top_brands.values, title=f"Top {num_top_brands} des marques", labels={"x": "Marque", "y": "Nombre de rappels"})
                 fig_brands.update_layout(xaxis_tickangle=-45)
                 st.plotly_chart(fig_brands, use_container_width=True)
             else: st.info("Aucune donn√©e de marque disponible dans les donn√©es filtr√©es.")

    with tab3:
        st.subheader("Recherche rapide dans les rappels affich√©s (champ: Motif)")
        
        # L'input de texte lui-m√™me met √† jour st.session_state.quick_search_input
        search_term = st.text_input(
            "Entrez un terme pour rechercher dans le Motif du rappel:",
            placeholder="Ex: list√©ria, salmonelle, corps √©tranger",
            value=st.session_state.quick_search_input, # Utilisez la valeur de l'√©tat pour initialiser le widget
            key="quick_search_input" # Cette cl√© g√®re l'√©tat automatiquement
        )
        # CORRECTION: Ligne redondante supprim√©e

        search_results_df = pd.DataFrame()
        # Appliquer la recherche si search_term n'est PAS vide (pour √©viter de tout afficher par d√©faut)
        if search_term:
            search_term_lower = search_term.lower()
            search_cols = ["motif"] # Recherche UNIQUEMENT sur le motif
            search_cols_existing = [col for col in search_cols if col in df_filtered.columns]
            
            if search_cols_existing:
                 df_filtered_with_search = df_filtered.copy()
                 # S'assurer que 'motif' est bien trait√© comme string avant de mettre en minuscule
                 # On acc√®de directement √† la colonne 'motif' car search_cols_existing ne contient que 'motif'
                 # S'assurer que la colonne 'motif' existe avant d'essayer d'y acc√©der
                 if 'motif' in df_filtered_with_search.columns:
                      df_filtered_with_search['search_text'] = df_filtered_with_search['motif'].astype(str).str.lower()
                 else:
                      # Si 'motif' n'existe pas (tr√®s improbable si load_rappel_data fonctionne), la recherche ne trouvera rien
                      df_filtered_with_search['search_text'] = "" 


                 search_results_df = df_filtered_with_search[df_filtered_with_search['search_text'].str.contains(search_term_lower, na=False)].copy()

            st.markdown(f"**{len(search_results_df)}** r√©sultats trouv√©s pour '{search_term}' dans le Motif.")

        # Afficher les r√©sultats SEULEMENT si search_term n'est PAS vide ET qu'il y a des r√©sultats
        if search_term and not search_results_df.empty:
            st.markdown("---")
            st.write("Navigation dans les r√©sultats de recherche:")

            items_per_page_search = st.select_slider("R√©sultats par page:", options=[5, 10, 20, 50], value=st.session_state.items_per_page_search, key="slider_items_per_page_search")
            st.session_state.items_per_page_search = items_per_page_search

            # Hacher la valeur actuelle du widget search_term
            current_pagination_state_search = hash((st.session_state.quick_search_input.lower() if st.session_state.quick_search_input else "", items_per_page_search))
            
            # R√©initialiser seulement si le terme de recherche (du widget) change (et il n'est pas vide), OU si la pagination_state change
            if (st.session_state.get("last_search_term_widget_value", "") != (st.session_state.quick_search_input.lower() if st.session_state.quick_search_input else "") and st.session_state.quick_search_input != "") or \
               ("pagination_state_search" not in st.session_state or st.session_state.pagination_state_search != current_pagination_state_search):
                 st.session_state.search_current_page = 1
                 st.session_state.pagination_state_search = current_pagination_state_search
            # Mettre √† jour le dernier terme du widget stock√© pour la prochaine comparaison
            st.session_state.last_search_term_widget_value = st.session_state.quick_search_input.lower() if st.session_state.quick_search_input else ""


            total_items_search = len(search_results_df)
            start_idx_search = (st.session_state.search_current_page - 1) * items_per_page_search
            end_idx_search = min(start_idx_search + items_per_page_search, total_items_search)

            # CORRECTION: Obtenir la sous-section du DataFrame pour la page actuelle de recherche
            # Utiliser .iloc pour la s√©lection par position, puis reset_index pour avoir des indices 0, 1, 2... sur la page
            # .copy() est une bonne pratique ici pour √©viter les SettingWithCopyWarning potentielles
            page_df_search = search_results_df.iloc[start_idx_search:end_idx_search].copy().reset_index(drop=True) 

            # Afficher en 2 colonnes en it√©rant sur les indices locaux de cette sous-section
            for i in range(0, len(page_df_search), 2):
                col1, col2 = st.columns(2)
                # Afficher les produits en utilisant l'index local 'i' de page_df_search
                with col1:
                     if i < len(page_df_search): # Double check index validity
                        display_recall_card(page_df_search.iloc[i])
                if i + 1 < len(page_df_search):
                    with col2:
                         display_recall_card(page_df_search.iloc[i+1])

            display_pagination_controls("search_current_page", total_items_search, items_per_page_search)

        # Afficher les messages d'info/warning seulement si search_term est vide ou s'il y a une recherche sans r√©sultat
        elif not search_term:
             st.info("Entrez un terme dans la barre de recherche pour afficher les r√©sultats (recherche limit√©e au Motif).")
        # Le cas else (search_term non vide mais search_results_df vide) est d√©j√† g√©r√© par le message "X r√©sultats trouv√©s pour..."

    with tab4:
        st.subheader("ü§ñ Analyse des donn√©es filtr√©es par l'IA Groq")
        # Afficher la r√©ponse IA stock√©e
        if st.session_state.ai_response:
            st.write("#### Question pos√©e :")
            # Afficher la derni√®re question pos√©e
            st.markdown(st.session_state.get("last_ai_question", "N/A")) 
            st.write("#### R√©ponse de l'IA :")
            # Afficher la r√©ponse de l'IA format√©e
            st.markdown(st.session_state.ai_response) 
        else:
            st.info("Aucune analyse IA n'a √©t√© effectu√©e. Entrez votre cl√© API et votre question dans la section 'Analyse IA' de la barre lat√©rale et cliquez sur 'Analyser avec l'IA'.")


    # Footer (identique)
    st.markdown("""
    <div class="footer">
        RappelConso Insight - Application bas√©e sur les donn√©es de <a href="https://www.rappelconso.gouv.fr/" target="_blank">RappelConso.gouv.fr</a>. Donn√©es fournies par data.economie.gouv.fr
    </div>
    """, unsafe_allow_html=True)


# Ex√©cuter l'application
if __name__ == "__main__":
    # Initialisation des √©tats de session (redondant avec main mais s√©curise)
    if "rappel_data" not in st.session_state: st.session_state.rappel_data = None
    if "load_params" not in st.session_state: st.session_state.load_params = None
    if "current_page" not in st.session_state: st.session_state.current_page = 1
    if "search_current_page" not in st.session_state: st.session_state.search_current_page = 1
    if "selected_subcategories" not in st.session_state: st.session_state.selected_subcategories = ["Toutes"]
    if "selected_brands" not in st.session_state: st.session_state.selected_brands = ["Toutes"]
    if "selected_risks" not in st.session_state: st.session_state.selected_risks = ["Tous"]
    if "quick_search_input" not in st.session_state: st.session_state.quick_search_input = ""
    # Correction ici : utiliser une cl√© diff√©rente pour l'√©tat de la valeur du widget
    if "last_search_term_widget_value" not in st.session_state:
         st.session_state.last_search_term_widget_value = ""
    if "items_per_page_main" not in st.session_state: st.session_state.items_per_page_main = 10
    if "items_per_page_search" not in st.session_state: st.session_state.items_per_page_search = 10
    if "groq_api_key" not in st.session_state: st.session_state.groq_api_key = ""
    # Correction ici : s'assurer que l'√©tat groq_model est un mod√®le valide d√®s l'initialisation
    if "groq_model" not in st.session_state or (st.session_state.groq_model is not None and st.session_state.groq_model not in GROQ_MODELS):
        st.session_state.groq_model = GROQ_MODELS[0] if GROQ_MODELS else None # Utiliser le premier mod√®le comme d√©faut si la liste n'est pas vide
    if "ai_question" not in st.session_state: st.session_state.ai_question = ""
    if "ai_response" not in st.session_state: st.session_state.ai_response = ""
    if "last_ai_question" not in st.session_state: st.session_state.last_ai_question = ""

    main()
